# From Linear Equation to Neural Network

<p align="center">
  <img src="media/thumbnail.png" alt="Project Thumbnail" width="700"/>
</p>

This project builds a neural network from scratch using only NumPy, starting from a basic linear equation (`y = wx + b`) and gradually stacking multiple such units to approximate nonlinear functions like `sin(x)`.

## ðŸ“º Watch the Project Demo

Check out the full walkthrough on YouTube:  
ðŸ”— [From Linear Equation to Neural Network â€“ Full Video](https://youtu.be/0iT1pXd9c9o)

## ðŸ“‡ Connect with Me

ðŸ”— [LinkedIn â€“ Soudeepan Biswas](https://www.linkedin.com/in/soudeepanbiswas/)

## ðŸ“Œ Highlights

- Built with just NumPy â€” no ML frameworks used.
- Understand how weights and biases form a neuron.
- Visualize learning by plotting predicted vs. actual `sin(x)`.
- Tanh activation and Xavier initialization.
- Manual gradient descent and weight updates.

## ðŸ§  Core Concepts

- Linear Equation (`y = wx + b`)
- Hidden Units
- Forward Pass
- Tanh Activation
- Manual Backpropagation
- Loss Minimization
- Sine Function Approximation


